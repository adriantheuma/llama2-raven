## Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance

<p>
    <a href="https://www.python.org/">
        <img alt="Build" src="https://img.shields.io/badge/Python-3.9+-1f425f.svg?color=blue">
    </a>
    <a href="https://github.com/adriantheuma/llama2-raven/blob/main/LICENCE">
        <img alt="License" src="https://img.shields.io/badge/License-MIT-blue">
    </a>
    <a href="https://huggingface.co/adriantheuma/raven-lora" target="_blank">
        <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20-Model-blue?color=blue&logoColor=white" />
    </a>
    <a href="https://huggingface.co/adriantheuma/raven-data" target="_blank">
        <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20-Data-blue?color=blue&logoColor=white" />
    </a>
</p>

## Overview

This repository is based on our publication *Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance* ([PDF](https://browse.arxiv.org/pdf/00000.pdf)). It contains data used to fine-tune the base [Llama 2 13B Chat](https://huggingface.co/meta-llama/Llama-2-13b) using LoRA. If you use this data in your work, please cite:

```
@misc{theuma2024raven,
      title={Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance}, 
      author={Adrian Theuma and Ehsan Shareghi},
      year={2023},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Data & Templates

The dataset that we use to fine-tune Raven is composed from four distinct question-answering datasets. Two are specifically from the financial domain with the remaining being generic and incorporating questions over both tables and text. 

-TAT-QA: Table-and-Text Question Answering consists of 16,552 questions generated by financial experts associated with 2,757 hybrid contexts drawn from real-world financial reports. 


\noindent\textbf{Financial PhraseBank}. Consists of 4,846 phrases derived from English financial news on listed companies in OMX Helsinki~\citep{DBLP:journals/jasis/MaloSKWT14}. The dataset contains phrase-level annotation by financial markets experts, that categorises each sample sentence exclusively from an investor's standpoint as either positive, negative, or neutral. This dataset is relevant for our research because sentiment analysis models trained on general datasets do not perform well in specialised domains due to the unique vocabulary found in financial texts, which often do not rely on easily identifiable positive or negative words~\citep{DBLP:journals/corr/abs-1908-10063}. We use phrases where at least two-thirds of the annotators agree on the sentiment, reducing the dataset to 4,217 phrases.


\noindent\textbf{Wiki-SQL}. Consists of 80,654 manually annotated crowd sourced examples of natural language questions and corresponding SQL queries over 24,241 tables found on Wikipedia~\citep{DBLP:journals/corr/abs-1709-00103}. Whilst this is not specifically a financial domain dataset its relevancy is in the availability of the script that produces the answer. Similar to the derivation in the TAT-QA dataset this script is crucial to steer our model to use a tool instead of producing the answer directly. All scripts and checked for syntax errors and we only keep instances where the result of the script matches the \emph{gold} answer. 


\noindent\textbf{OTT-QA}. Similar to \textsc{TAT-QA}, Open Table-and-Text Question Answering consists of 43,683 questions over tabular data and unstructured text across diverse domains~\citep{DBLP:conf/iclr/ChenCSWC21}. The majority of questions necessitate multi-hop inference involving both forms of data. The dataset's primary relevance lies in the lack of intermediate steps for retrieving the evidence needed and/ or a derivation to determine the answer to the question. This absence poses a challenge for the model to produce correct answers.
%\textbf{\textsc{Alpaca} instruction-tuning dataset}. Consists of 52K instruction, input (optional), output triplets~\citep{alpaca} generated using the self-instruct protocol introduced by~\citet{DBLP:conf/acl/WangKMLSKH23}. We hypothesise that incorporating this open-domain dataset within the training dataset preserves the foundational language processing capabilities of the base model after fine-tuning.

\subsection{Data preparation}\label{subsec:raven_dataprep}
The datasets described above have diverse formats and are not suited for fine-tuning \textsc{Raven} as-is. We employ a data conversion pipeline to convert these four datasets into a homogeneous dataset suitable to fine-tune our financial model. In general, we require to extract up to four key attributes from the original datasets. These are (1) \emph{instruction} that describes the task to perform, for example, "\emph{Determine the sentiment of the following phrase}", or the question "\emph{What is the percentage change in revenue after the adoption of ASC 606?}" (2) \emph{input} that provides more context such as the phrase to classify or a passage, (3) \emph{data} that accompanies the context, in tabular format, (4) \emph{derivation} that produces the answer or expected \emph{response}. The instruction and one of derivation or response are mandatory, whilst the other attributes are included if applicable. Refer to Appendix \ref{app:templates} for the full prompt. 


To obtain a balanced dataset~\footnote{available at https://bit.ly/45JHY9X} we randomly sub-sample larger datasets such that we obtain a uniformly distributed dataset among the different sources. As summarised in Table \ref{tab:datasets}, the size of the final dataset is 45,477 samples of which 4,050 samples are used for validation during the training loop. The size of the test set is 4,482 observations. 